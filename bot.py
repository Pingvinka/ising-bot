import numpy as np
import torch
import torch.nn as nn
import math
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import asyncio
import io
import hashlib
import nest_asyncio
import os
import sys

# –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ Jupyter/Colab
nest_asyncio.apply()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
import logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

def set_seeds(seed=42): #–°–∏–¥—ã
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seeds(42)

class AdvancedGNN(nn.Module):
    def __init__(self, n_nodes, hidden=256):
        super().__init__()
        self.n_nodes = n_nodes

        self.encoder = nn.Sequential(
            nn.Linear(6, hidden),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden, hidden),
            nn.GELU(),
            nn.Dropout(p=0.1),
            nn.Linear(hidden, hidden),
            nn.GELU(),
        )

        self.policy_head = nn.Sequential(
            nn.Linear(hidden * 2, hidden),
            nn.GELU(),
            nn.Dropout(p=0.1),
            nn.Linear(hidden, 1)
        )

        self.stop_head = nn.Sequential(
            nn.Linear(hidden, hidden // 2),
            nn.GELU(),
            nn.Linear(hidden // 2, 1)
        )

        self.value_head = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.GELU(),
            nn.Dropout(p=0.1),
            nn.Linear(hidden, hidden // 2),
            nn.GELU(),
            nn.Dropout(p=0.1),
            nn.Linear(hidden // 2, 1)
        )

    def forward(self, x):
        batch_size = x.shape[0]
        x = x.view(batch_size, self.n_nodes, -1)

        h = self.encoder(x)
        global_emb = h.mean(dim=1)

        global_expanded = global_emb.unsqueeze(1).expand(-1, self.n_nodes, -1)
        node_features = torch.cat([h, global_expanded], dim=-1)
        node_logits = self.policy_head(node_features).squeeze(-1)

        stop_logit = self.stop_head(global_emb).squeeze(-1)
        logits = torch.cat([node_logits, stop_logit.unsqueeze(1)], dim=1)
        value = self.value_head(global_emb).squeeze(-1)

        return logits, value

class AdvancedIsingEnv:
    def __init__(self, adj_matrix, seed=42):
        self.J = adj_matrix.astype(np.float32)
        self.N = len(adj_matrix)
        self.np_random = np.random.RandomState(seed)
        self.reset()

    def reset(self):
        self.spins = self.np_random.choice([-1, 1], self.N)
        self.best_spins = self.spins.copy()
        self.best_energy = self.energy(self.spins)
        self.steps = 0
        self.no_improvement_steps = 0
        self.consecutive_flips = np.zeros(self.N, dtype=int)
        return self._get_obs()

    def step(self, action):
        self.steps += 1
        self.consecutive_flips += 1

        if action == self.N: 
            return self._get_obs(), 0, True, {
                "energy": self.energy(self.spins),
                "best_energy": self.best_energy
            }

        self.consecutive_flips[action] = 0
        old_energy = self.energy(self.spins)
        self.spins[action] *= -1
        new_energy = self.energy(self.spins)

        reward = old_energy - new_energy

        if new_energy < self.best_energy:
            self.best_spins = self.spins.copy()
            self.best_energy = new_energy
            self.no_improvement_steps = 0
            reward += 1.0
        else:
            self.no_improvement_steps += 1
            reward -= 0.1

        done = (self.steps >= self.N * 3 or
                self.no_improvement_steps >= self.N * 2)

        return self._get_obs(), reward, done, {
            "energy": new_energy,
            "best_energy": self.best_energy
        }

    def energy(self, spins):
        return -0.5 * np.sum(self.J * np.outer(spins, spins))

    def _get_obs(self):
        local_field = self.J @ self.spins
        delta_energy = 2 * self.spins * local_field
        deg_pos = (self.J == 1).sum(axis=1)
        deg_neg = (self.J == -1).sum(axis=1)

        return np.column_stack([
            self.spins,
            local_field,
            delta_energy,
            deg_pos / self.N,
            deg_neg / self.N,
            self.consecutive_flips / 10.0
        ]).astype(np.float32)

def predict_spins_deterministic(agent, adj_matrix, n_restarts=10):
    best_energy = float('inf')
    best_spins = None

    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ—Å—Ç–∞—Ä—Ç–∞
    seeds = [42 + i * 100 for i in range(n_restarts)]

    for restart, seed in enumerate(seeds):
        set_seeds(seed)

        env = AdvancedIsingEnv(adj_matrix, seed=seed)
        state = env.reset()
        local_best_spins = env.best_spins.copy()
        local_best_energy = env.best_energy

        for step in range(env.N * 30):
            state_t = torch.FloatTensor(state).unsqueeze(0)

            with torch.no_grad():
                logits, _ = agent(state_t)
                action = torch.argmax(logits, dim=-1).item()

            state, _, done, info = env.step(action)

            if info['best_energy'] < local_best_energy:
                local_best_energy = info['best_energy']
                local_best_spins = env.best_spins.copy()

            if done or action == env.N:
                break

        if local_best_energy < best_energy:
            best_energy = local_best_energy
            best_spins = local_best_spins
    print(best_energy)
    return best_spins

def load_model(model_path, n_spins):
    try:
        logger.info(f"üîÑ Attempting to load model from: {model_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(model_path):
            logger.error(f"‚ùå Model file not found: {model_path}")
            logger.info(f"üìÅ Current directory: {os.getcwd()}")
            logger.info(f"üìÅ Directory contents: {os.listdir('.')}")
            if os.path.exists('models'):
                logger.info(f"üìÅ Models directory contents: {os.listdir('models')}")
            return None
            
        logger.info("‚úÖ Model file exists")
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
        try:
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
            logger.info("‚úÖ Model loaded with weights_only=False")
        except Exception as e1:
            logger.warning(f"‚ö†Ô∏è First load attempt failed: {e1}")
            try:
                checkpoint = torch.load(model_path, map_location='cpu')
                logger.info("‚úÖ Model loaded with default parameters")
            except Exception as e2:
                logger.error(f"‚ùå Error loading model: {e2}")
                return None
        
        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
        agent = AdvancedGNN(n_spins)
        logger.info("‚úÖ AdvancedGNN model created")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É checkpoint
        logger.info(f"üìÅ Checkpoint keys: {list(checkpoint.keys())}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        if 'agent' in checkpoint:
            agent.load_state_dict(checkpoint['agent'])
            logger.info("‚úÖ Model weights loaded from 'agent' key")
        elif 'model_state_dict' in checkpoint:
            agent.load_state_dict(checkpoint['model_state_dict'])
            logger.info("‚úÖ Model weights loaded from 'model_state_dict' key")
        elif 'state_dict' in checkpoint:
            agent.load_state_dict(checkpoint['state_dict'])
            logger.info("‚úÖ Model weights loaded from 'state_dict' key")
        else:
            try:
                agent.load_state_dict(checkpoint)
                logger.info("‚úÖ Model weights loaded directly from checkpoint")
            except Exception as e:
                logger.error(f"‚ùå Could not load model weights: {e}")
                return None
        
        agent.eval()
        logger.info("‚úÖ Model set to eval mode")
        return agent
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load model: {e}")
        return None

def read_matrix_from_file(file_content):
    lines = file_content.decode('utf-8').strip().split('\n')
    matrix = []
    for line in lines:
        row = [float(x) for x in line.strip().split()]
        matrix.append(row)
    return np.array(matrix)

def save_spins_to_file(spins):
    output = io.StringIO()
    for spin in spins:
        output.write(f"{spin}\n")
    return output.getvalue()

results_cache = {}

def get_matrix_hash(matrix):
    return hashlib.md5(matrix.tobytes()).hexdigest()

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –ò–∑–∏–Ω–≥–∞ :>\n"
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ñ–∞–π–ª .txt —Å –º–∞—Ç—Ä–∏—Ü–µ–π —Å–º–µ–∂–Ω–æ—Å—Ç–∏, –∏ —è –≤–µ—Ä–Ω—É –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å–ø–∏–Ω—ã.\n\n"
    )

async def handle_file(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if 'agent' not in context.bot_data or context.bot_data['agent'] is None:
            await update.message.reply_text("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ë–æ—Ç –Ω–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª—ã.")
            return
            
        file = await update.message.document.get_file()
        file_content = await file.download_as_bytearray()

        await update.message.reply_text("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –º–∞—Ç—Ä–∏—Ü—É...")

        adj_matrix = read_matrix_from_file(file_content)
        n_spins = len(adj_matrix)

        expected_n_spins = context.bot_data['n_spins']
        if n_spins != expected_n_spins:
            await update.message.reply_text(
                f"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã ({n_spins}) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–º—É "
                f"({expected_n_spins}). –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ."
            )
            return

        matrix_hash = get_matrix_hash(adj_matrix)
        if matrix_hash in results_cache:
            spins = results_cache[matrix_hash]
            cache_info = " (–∏–∑ –∫—ç—à–∞)"
        else:
            agent = context.bot_data['agent']
            spins = predict_spins_deterministic(agent, adj_matrix, n_restarts=8)
            results_cache[matrix_hash] = spins
            cache_info = ""

        result_text = save_spins_to_file(spins)

        result_file = io.BytesIO(result_text.encode('utf-8'))
        result_file.name = f"spins_{n_spins}.txt"

        energy = -0.5 * np.sum(adj_matrix * np.outer(spins, spins))

        await update.message.reply_text(
            f"–†–µ—à–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ{cache_info}!\n"
            f"–≠–Ω–µ—Ä–≥–∏—è: {energy:.2f}\n"
        )
        await update.message.reply_document(document=result_file)

    except Exception as e:
        logger.error(f"Error handling file: {e}")
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:\n\n"
        "1. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ —Ñ–∞–π–ª .txt —Å —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–π –º–∞—Ç—Ä–∏—Ü–µ–π —Å–º–µ–∂–Ω–æ—Å—Ç–∏\n"
        "2. –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–∑–º–µ—Ä–æ–º 200√ó200\n"
        "3. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —á–∏—Å–ª–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª–∞–º–∏\n"
        "4. –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª –±–æ—Ç—É\n\n"
        "–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞:\n"
        "0 1 -1 0 ...\n"
        "1 0 0 -1 ...\n"
        "-1 0 0 1 ...\n"
        "0 -1 1 0 ...\n"
        "...\n\n"
      
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É\n"
        "/help - –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É\n"
        "/tea - –≤—ã–ø–∏—Ç—å —á–∞–π ‚òï"
    )

async def clear_cache(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global results_cache
    old_size = len(results_cache)
    results_cache = {}
    await update.message.reply_text(f"üßπ –ö—ç—à –æ—á–∏—â–µ–Ω! –£–¥–∞–ª–µ–Ω–æ {old_size} –∑–∞–ø–∏—Å–µ–π.")

async def tea_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        photo_path = "assets/peite_chai.jpg"  
        
        if not os.path.exists(photo_path):
            await update.message.reply_text("–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —á–∞–π–Ω–∏–∫ —Å–ª–æ–º–∞–ª—Å—è... –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return
        
        with open(photo_path, 'rb') as photo:
            await update.message.reply_photo(
                photo=photo,
                caption="‚òï –í–æ—Ç –≤–∞—à —á–∞–π! –ü—Ä–∏—è—Ç–Ω–æ–≥–æ —á–∞–µ–ø–∏—Ç–∏—è! üçµ\n\n"
                       "–ü–æ–∫–∞ –ø—å—ë—Ç–µ —á–∞–π, –º–æ–∂–µ—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–Ω–µ –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –ò–∑–∏–Ω–≥–∞! üòä"
            )
    except Exception as e:
        await update.message.reply_text(f"‚ùå –ß–∞–π–Ω–∏–∫ –∑–∞–∫–∏–ø–µ–ª —Å –æ—à–∏–±–∫–æ–π: {str(e)}")

async def main_async():
    try:
        logger.info("üöÄ Starting bot initialization...")
        
        TOKEN = "8481020311:AAFtFAzgahTdfX3kB3uA1ySefGFtn6_VjYk"  
        MODEL_PATH = "models/best_ising_model_ppg.pth"
        N_SPINS = 200

        logger.info(f"üîß Configuration: TOKEN={TOKEN[:10]}..., MODEL_PATH={MODEL_PATH}, N_SPINS={N_SPINS}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
        logger.info(f"üìÅ Current working directory: {os.getcwd()}")
        logger.info(f"üìÅ Directory contents: {os.listdir('.')}")
        
        if os.path.exists('models'):
            logger.info(f"üìÅ Models directory contents: {os.listdir('models')}")
        else:
            logger.warning("‚ö†Ô∏è Models directory does not exist!")

        logger.info("üîÑ Loading model...")
        agent = load_model(MODEL_PATH, N_SPINS)
        
        if agent is None:
            logger.error("‚ùå Failed to load model. Bot cannot start.")
            return

        logger.info("‚úÖ Model loaded successfully!")
        
        logger.info("üîß Creating bot application...")
        application = Application.builder().token(TOKEN).build()

        application.bot_data['agent'] = agent
        application.bot_data['n_spins'] = N_SPINS

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        application.add_handler(CommandHandler("start", start))
        application.add_handler(CommandHandler("help", help_command))
        application.add_handler(CommandHandler("clear_cache", clear_cache))
        application.add_handler(CommandHandler("tea", tea_command))
        application.add_handler(MessageHandler(filters.Document.TXT, handle_file))

        logger.info("ü§ñ Starting bot polling...")
        await application.run_polling(
            drop_pending_updates=True,
            allowed_updates=Update.ALL_TYPES,
            timeout=30
        )
        
    except Exception as e:
        logger.error(f"üí• Critical error in main_async: {e}")
        raise

if __name__ == "__main__":
    logger.info("üéØ Script started")
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Bot stopped by user")
    except Exception as e:
        logger.error(f"üí• Fatal error: {e}")
    finally:
        logger.info("üèÅ Script finished")
